{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dde49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e205a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>WINDSPEED</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TOTALDEMAND</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>MIN</th>\n",
       "      <th>MAX</th>\n",
       "      <th>SOLAR</th>\n",
       "      <th>TEMPAVE</th>\n",
       "      <th>RRP</th>\n",
       "      <th>FORECASTDEMAND</th>\n",
       "      <th>OUTPUT</th>\n",
       "      <th>MONTHDATE</th>\n",
       "      <th>WEEKDAY</th>\n",
       "      <th>WEEKEND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656341</td>\n",
       "      <td>15.902439</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>6853.633437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>32.2</td>\n",
       "      <td>21.95</td>\n",
       "      <td>38.472917</td>\n",
       "      <td>6665.366167</td>\n",
       "      <td>23.465</td>\n",
       "      <td>01-2016</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.656341</td>\n",
       "      <td>15.902439</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6727.613958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>21.00</td>\n",
       "      <td>36.907292</td>\n",
       "      <td>6236.849955</td>\n",
       "      <td>23.465</td>\n",
       "      <td>01-2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.688837</td>\n",
       "      <td>14.488372</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>6616.406076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>25.6</td>\n",
       "      <td>10.3</td>\n",
       "      <td>21.55</td>\n",
       "      <td>31.997083</td>\n",
       "      <td>6551.924748</td>\n",
       "      <td>23.465</td>\n",
       "      <td>01-2016</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.679545</td>\n",
       "      <td>22.477273</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>7367.750278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>23.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.90</td>\n",
       "      <td>33.424583</td>\n",
       "      <td>6729.993123</td>\n",
       "      <td>23.465</td>\n",
       "      <td>01-2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.768837</td>\n",
       "      <td>22.581395</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>7462.242014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>20.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>19.05</td>\n",
       "      <td>33.053958</td>\n",
       "      <td>7333.898202</td>\n",
       "      <td>23.465</td>\n",
       "      <td>01-2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  HUMIDITY  WINDSPEED        DATE  TOTALDEMAND  HOLIDAY  \\\n",
       "0  2016      1    1  0.656341  15.902439  2016-01-01  6853.633437      2.0   \n",
       "1  2016      1    2  0.656341  15.902439  2016-01-02  6727.613958      0.0   \n",
       "2  2016      1    3  0.688837  14.488372  2016-01-03  6616.406076      0.0   \n",
       "3  2016      1    4  0.679545  22.477273  2016-01-04  7367.750278      0.0   \n",
       "4  2016      1    5  0.768837  22.581395  2016-01-05  7462.242014      0.0   \n",
       "\n",
       "    MIN   MAX  SOLAR  TEMPAVE        RRP  FORECASTDEMAND  OUTPUT MONTHDATE  \\\n",
       "0  15.3  28.6   32.2    21.95  38.472917     6665.366167  23.465   01-2016   \n",
       "1  15.9  26.1   21.7    21.00  36.907292     6236.849955  23.465   01-2016   \n",
       "2  17.5  25.6   10.3    21.55  31.997083     6551.924748  23.465   01-2016   \n",
       "3  18.2  23.6    6.4    20.90  33.424583     6729.993123  23.465   01-2016   \n",
       "4  17.6  20.5    4.4    19.05  33.053958     7333.898202  23.465   01-2016   \n",
       "\n",
       "   WEEKDAY  WEEKEND  \n",
       "0        4        0  \n",
       "1        5        1  \n",
       "2        6        1  \n",
       "3        0        0  \n",
       "4        1        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset into a pandas dataframe\n",
    "# data = pd.read_csv(\"../data/Cleaned data/data.csv\") \n",
    "data = pd.read_csv(\"data.csv\") \n",
    "\n",
    "data.dropna(inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f33009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df before join operation: 2325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = data.drop(['MIN','MAX','FORECASTDEMAND', 'MONTHDATE','WEEKEND'], axis=1)\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "#print(df.isna().sum())\n",
    "# Check number of rows in df before join operation\n",
    "print(\"Number of rows in df before join operation:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475d1dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>WINDSPEED</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TOTALDEMAND</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>SOLAR</th>\n",
       "      <th>TEMPAVE</th>\n",
       "      <th>RRP</th>\n",
       "      <th>OUTPUT</th>\n",
       "      <th>MON</th>\n",
       "      <th>TUE</th>\n",
       "      <th>WED</th>\n",
       "      <th>THU</th>\n",
       "      <th>FRI</th>\n",
       "      <th>SAT</th>\n",
       "      <th>SUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656341</td>\n",
       "      <td>15.902439</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>6853.633437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>21.95</td>\n",
       "      <td>38.472917</td>\n",
       "      <td>23.465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.656341</td>\n",
       "      <td>15.902439</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6727.613958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.7</td>\n",
       "      <td>21.00</td>\n",
       "      <td>36.907292</td>\n",
       "      <td>23.465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.688837</td>\n",
       "      <td>14.488372</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>6616.406076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>21.55</td>\n",
       "      <td>31.997083</td>\n",
       "      <td>23.465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.679545</td>\n",
       "      <td>22.477273</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>7367.750278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.90</td>\n",
       "      <td>33.424583</td>\n",
       "      <td>23.465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.768837</td>\n",
       "      <td>22.581395</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>7462.242014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>19.05</td>\n",
       "      <td>33.053958</td>\n",
       "      <td>23.465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  HUMIDITY  WINDSPEED        DATE  TOTALDEMAND  HOLIDAY  \\\n",
       "0  2016      1    1  0.656341  15.902439  2016-01-01  6853.633437      2.0   \n",
       "1  2016      1    2  0.656341  15.902439  2016-01-02  6727.613958      0.0   \n",
       "2  2016      1    3  0.688837  14.488372  2016-01-03  6616.406076      0.0   \n",
       "3  2016      1    4  0.679545  22.477273  2016-01-04  7367.750278      0.0   \n",
       "4  2016      1    5  0.768837  22.581395  2016-01-05  7462.242014      0.0   \n",
       "\n",
       "   SOLAR  TEMPAVE        RRP  OUTPUT  MON  TUE  WED  THU  FRI  SAT  SUN  \n",
       "0   32.2    21.95  38.472917  23.465  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "1   21.7    21.00  36.907292  23.465  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "2   10.3    21.55  31.997083  23.465  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "3    6.4    20.90  33.424583  23.465  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    4.4    19.05  33.053958  23.465  0.0  1.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Onehot Encoding for categorial data (Weekday)\n",
    "\n",
    "\n",
    "# Select the \"WEEKDAY\" column and create a new dataframe\n",
    "weekday_df = df[['WEEKDAY']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a one-hot encoder object\n",
    "\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "\n",
    "# Fit and transform the weekday data\n",
    "weekday_encoded = encoder.fit_transform(df[['WEEKDAY']]).toarray()\n",
    "\n",
    "# Create a new dataframe with the encoded weekday data\n",
    "weekday_df_encoded = pd.DataFrame(weekday_encoded, columns=['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Drop weekday column\n",
    "df = df.drop(['WEEKDAY'], axis=1)\n",
    "\n",
    "\n",
    "# Concatenate the original dataframe with the encoded weekday dataframe\n",
    "df = pd.concat([df.reset_index(drop=True), weekday_df_encoded], axis=1)\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e17d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to a datetime object\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "# Set 'DATE' as the index\n",
    "df.set_index('DATE', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bae8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets. \n",
    "\n",
    "train_set = np.where((df.index >= datetime(2017, 8, 1)) & (df.index < datetime(2021, 8, 1)))[0]\n",
    "test_set = np.where(df.index >= datetime(2021, 8, 1))[0]\n",
    "\n",
    "\n",
    "# identify response variable and predictors\n",
    "\n",
    "X = df.drop(['TOTALDEMAND'], axis=1).values\n",
    "y = df['TOTALDEMAND'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64589086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.63636364 0.         0.54515261 0.48699955 0.\n",
      "  0.38782051 0.18563923 0.7116133  0.10793701 0.         1.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.63636364 0.03333333 0.54515261 0.48699955 0.\n",
      "  0.38461538 0.18739054 0.64976244 0.10793701 0.         0.\n",
      "  1.         0.         0.         0.         0.        ]\n",
      " [0.         0.63636364 0.06666667 0.67166992 0.28363047 0.\n",
      "  0.25961538 0.26795096 0.749493   0.10793701 0.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.63636364 0.1        0.86132831 0.12704281 0.\n",
      "  0.39423077 0.22767075 0.68692824 0.10793701 0.         0.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.63636364 0.13333333 0.48354039 0.44853191 0.\n",
      "  0.41025641 0.2084063  0.55983012 0.10793701 0.         0.\n",
      "  0.         0.         0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "\n",
    "\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler_X.fit_transform(X[train_set])\n",
    "X_test_scaled = scaler_X.transform(X[test_set])\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(y[train_set].reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler_y.transform(y[test_set].reshape(-1, 1)).ravel()\n",
    "\n",
    "print(X_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43a8baf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.63636364, 0.63636364, 0.63636364, 0.63636364, 0.63636364,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.81818182, 0.81818182, 0.81818182, 0.81818182,\n",
       "       0.81818182, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,\n",
       "       0.90909091, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.18181818, 0.18181818, 0.18181818,\n",
       "       0.18181818, 0.18181818, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.27272727, 0.27272727, 0.27272727,\n",
       "       0.27272727, 0.27272727, 0.36363636, 0.36363636, 0.36363636,\n",
       "       0.36363636, 0.36363636, 0.36363636, 0.36363636, 0.36363636,\n",
       "       0.36363636, 0.36363636, 0.36363636, 0.36363636, 0.36363636,\n",
       "       0.36363636, 0.36363636, 0.36363636, 0.36363636, 0.36363636,\n",
       "       0.36363636, 0.36363636, 0.36363636, 0.36363636, 0.36363636,\n",
       "       0.36363636, 0.45454545, 0.45454545, 0.45454545, 0.45454545,\n",
       "       0.45454545, 0.45454545, 0.45454545, 0.45454545, 0.45454545,\n",
       "       0.45454545, 0.45454545, 0.54545455, 0.54545455, 0.54545455,\n",
       "       0.54545455, 0.54545455, 0.54545455, 0.54545455, 0.54545455,\n",
       "       0.54545455, 0.54545455, 0.54545455, 0.54545455, 0.54545455,\n",
       "       0.54545455, 0.54545455, 0.54545455, 0.54545455, 0.54545455,\n",
       "       0.54545455, 0.54545455, 0.54545455, 0.54545455, 0.54545455,\n",
       "       0.54545455, 0.54545455, 0.63636364])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshape the data for input to the LSTM model\n",
    "#(This step is fit the data to 3D tensor format for LSTM model to process the sequential data efficiently \n",
    "#and capture any temporal dependencies in the data)\n",
    "\n",
    "#using sliding window approach to create input-output pairs with timesteps n = 1\n",
    "\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(1, len(X_train_scaled)):\n",
    "    X_train.append(X_train_scaled[i-1:i, :])\n",
    "    y_train.append(X_train_scaled[i, 1])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(1, len(X_test_scaled)):\n",
    "    X_test.append(X_test_scaled[i-1:i, :])\n",
    "    y_test.append(X_test_scaled[i, 1])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_train.shape[2]))\n",
    "\n",
    "\n",
    "#X_train.shape[0]\n",
    "#X_train.shape[1]\n",
    "#X_train.shape[2]\n",
    "\n",
    "y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a22cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model with three LSTM layers and one Dense output layer\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=64, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc41dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45/45 [==============================] - 3s 12ms/step - loss: 0.3297 - val_loss: 0.3158\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2855 - val_loss: 0.2611\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2275 - val_loss: 0.1903\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - val_loss: 0.1226\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 0.0976\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0861 - val_loss: 0.0928\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 0.0853\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0769\n",
      "Epoch 9/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.0678\n",
      "Epoch 10/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0588\n",
      "Epoch 11/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.0501\n",
      "Epoch 12/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.0416\n",
      "Epoch 13/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0338\n",
      "Epoch 14/50\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.0287 - val_loss: 0.0277\n",
      "Epoch 15/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0231 - val_loss: 0.0221\n",
      "Epoch 16/50\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0178\n",
      "Epoch 17/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 18/50\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 19/50\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0129\n",
      "Epoch 20/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 21/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 22/50\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 23/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0087\n",
      "Epoch 24/50\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0086\n",
      "Epoch 25/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0097 - val_loss: 0.0069\n",
      "Epoch 26/50\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0073\n",
      "Epoch 27/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 0.0071\n",
      "Epoch 28/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0092 - val_loss: 0.0059\n",
      "Epoch 29/50\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0061\n",
      "Epoch 30/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0085 - val_loss: 0.0059\n",
      "Epoch 31/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0057\n",
      "Epoch 32/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0052\n",
      "Epoch 33/50\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0056\n",
      "Epoch 34/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0052\n",
      "Epoch 35/50\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0050\n",
      "Epoch 36/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.0050\n",
      "Epoch 37/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0078 - val_loss: 0.0050\n",
      "Epoch 38/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0051\n",
      "Epoch 39/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0053\n",
      "Epoch 40/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0046\n",
      "Epoch 41/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0079 - val_loss: 0.0049\n",
      "Epoch 42/50\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.0075 - val_loss: 0.0052\n",
      "Epoch 43/50\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0046\n",
      "Epoch 44/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0049\n",
      "Epoch 45/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.0055\n",
      "Epoch 46/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0046\n",
      "Epoch 47/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0074 - val_loss: 0.0045\n",
      "Epoch 48/50\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0047\n",
      "Epoch 49/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0071 - val_loss: 0.0044\n",
      "Epoch 50/50\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0075 - val_loss: 0.0044\n"
     ]
    }
   ],
   "source": [
    "# Traing the LSTM model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee788f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c151964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0044\n",
      "R-squared: 0.9587\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE and R-squared\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE: %.4f' % mse)\n",
    "print('R-squared: %.4f' % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9526bb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "\n",
    "# Plot training and validation loss (MSE) for each epoch\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdea69e",
   "metadata": {},
   "source": [
    "Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc25f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75be3bc",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db7d88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0525f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combat shap.DeepExplainer issue (did not work)\n",
    "# import tensorflow as tf    \n",
    "# tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "944a1500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 247, in grad_graph  *\n        out = self.model(shap_rAnD)\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 378, in custom_grad\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 667, in handler\n        return linearity_with_excluded_handler(input_inds, explainer, op, *grads)\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 674, in linearity_with_excluded_handler\n        assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 224, in _variable_inputs\n        out[i] = t.name in self.between_tensors\n\n    AttributeError: Exception encountered when calling layer 'lstm_1' (type LSTM).\n    \n    'TFDeep' object has no attribute 'between_tensors'\n    \n    Call arguments received by layer 'lstm_1' (type LSTM):\n       inputs=tf.Tensor(shape=(2832, 1, 128), dtype=float32)\n       mask=None\n       training=False\n       initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b6ad3a099bae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;34m\"top\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \"\"\"\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[1;31m# run attribution computation graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0msample_phis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi_symbolic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoint_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_with_overridden_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mexecute_with_overridden_gradients\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;31m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;31m# reinstate the backpropagatable check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36manon\u001b[1;34m()\u001b[0m\n\u001b[0;32m    363\u001b[0m                     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[0mfinal_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                     \u001b[0mtf_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_backprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mtf__grad_graph\u001b[1;34m(shap_rAnD)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_rAnD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_rAnD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mcustom_grad\u001b[1;34m(self, op, *grads)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[0mtype_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shap_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop_handlers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we cut off the shap_ prefex before the lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mhandler\u001b[1;34m(explainer, op, *grads)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlinearity_with_excluded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_inds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlinearity_with_excluded_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_inds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36mlinearity_with_excluded_handler\u001b[1;34m(input_inds, explainer, op, *grads)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_inds\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_inds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"th input to \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" cannot vary!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shap_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\u001b[0m in \u001b[0;36m_variable_inputs\u001b[1;34m(self, op)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                 \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbetween_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vinputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vinputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 247, in grad_graph  *\n        out = self.model(shap_rAnD)\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 378, in custom_grad\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 667, in handler\n        return linearity_with_excluded_handler(input_inds, explainer, op, *grads)\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 674, in linearity_with_excluded_handler\n        assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n    File \"C:\\Users\\c-b-2\\anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 224, in _variable_inputs\n        out[i] = t.name in self.between_tensors\n\n    AttributeError: Exception encountered when calling layer 'lstm_1' (type LSTM).\n    \n    'TFDeep' object has no attribute 'between_tensors'\n    \n    Call arguments received by layer 'lstm_1' (type LSTM):\n       inputs=tf.Tensor(shape=(2832, 1, 128), dtype=float32)\n       mask=None\n       training=False\n       initial_state=None\n"
     ]
    }
   ],
   "source": [
    "# explain the model's predictions using SHAP values\n",
    "\n",
    "# Please note, error when running shap_values = explainer.shap_values(X_test)\n",
    "# error: 'TFDeep' object has no attribute 'between_tensors'\n",
    "\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the first prediction's explanation\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training set predictions\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac14497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30106e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc221",
   "metadata": {},
   "source": [
    "### Supporting resources\n",
    "shap.DeepExplainer\n",
    "https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html\n",
    "\n",
    "SHAP - Neural networks - Census income classification with Keras\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/neural_networks/Census%20income%20classification%20with%20Keras.html\n",
    "\n",
    "SHAP - Keras LSTM for IMDB Sentiment Classification\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html\n",
    "\n",
    "Kaggle - SHAP for LSTM\n",
    "https://www.kaggle.com/code/phamvanvung/shap-for-lstm/notebook\n",
    "\n",
    "[forecast][LSTM+SHAP]Applied SHAP on the polynomial equation case with LSTM algorithm\n",
    "https://medium.com/@sakamoto2000.kim/applied-shap-on-the-polynomial-equation-case-with-lstm-algorithm-7c140d15736b\n",
    "\n",
    "#### Troubleshooting error when running \"shap_values = explainer.shap_values(X_test)\", error: 'TFDeep' object has no attribute 'between_tensors'\n",
    "https://github.com/slundberg/shap/issues/2808\n",
    "\n",
    "https://www.pythonfixing.com/2021/12/fixed-shap-deepexplainer-with.html\n",
    "\n",
    "https://stackoverflow.com/questions/66814523/shap-deepexplainer-with-tensorflow-2-4-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c6162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d863c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031f2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adf377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
